# -*- coding: utf-8 -*-
"""Apple_CNN With Predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dD2W-M7ru9HZ8K4elk-S0plGxhIl6bdX
"""

import numpy as np
import os
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
import sklearn
from tensorflow.keras import datasets, layers, models
import pathlib

"""# Load dataset from URL

MinnieApple web page and dataset can be found here: https://conservancy.umn.edu/handle/11299/206575
"""

dataset_url = "https://conservancy.umn.edu/bitstream/handle/11299/206575/counting.tar.gz"

data_dir = tf.keras.utils.get_file(origin=dataset_url, 
                                   fname='counting', 
                                   untar=True)
data_dir = pathlib.Path(data_dir)

"""# Display sub folders"""

for subfolder in os.walk(data_dir):
  for subsubfolder in subfolder:
    print(subsubfolder)
    break

"""# Load n images into array (non-sorted)"""

def load_images_from_folder(folder, number_of_images_to_load):
  count = 1
  images = []
  dir = os.listdir(folder)
  order = []
  names = []
  if number_of_images_to_load == -1:
    number_of_images = next(os.walk(folder))[2]
    number_of_images_to_load = len(number_of_images)

  for filename in dir[:number_of_images_to_load]:
      names.append(filename)

      idx = filename.split("_")[1]
      idx = idx.split(".")[0]
      order.append(idx)

      img = cv2.imread(folder+"/"+filename)
      images.append(img)

      if number_of_images_to_load != -1:
        if count >= number_of_images_to_load:
          print("Max images reached")
          break
        else:
          count = count +1

  return [images,names,order]

"""# Load training data"""

max_images =-1

train_path = f"{str(data_dir)}/train/images"

train_data = load_images_from_folder(train_path,max_images)

train_order = np.asarray(train_data[2])
train_order = train_order.astype(int)

train_names = np.asarray(train_data[1])

print(len(train_data[0]))

"""# Load Validation data"""

val_path = f"{str(data_dir)}/val/images"

val_data = load_images_from_folder(val_path,max_images)

val_order = np.asarray(val_data[2])
val_order = val_order.astype(int)

val_names = np.asarray(val_data[1])

print(len(val_data[0]))

"""# Load Test data"""

test_path = f"{str(data_dir)}/test/images"

test_data = load_images_from_folder(test_path,max_images)

print(len(test_data[0]))

"""# Load all labels into array (sorted according to data order)

# Training Labels
"""

train_label_path = "/root/.keras/datasets/counting/train/train_ground_truth.txt"

train_labels = []

train_file = open(train_label_path, 'r') 
Lines = train_file.readlines() 

for line in Lines[1:]:
  train_labels.append(int(line[-2])) 

train_labels = np.asarray(train_labels)
train_labels = train_labels[train_order] 

print(train_labels)

"""# Validation Labels"""

val_label_path = "/root/.keras/datasets/counting/val/val_ground_truth.txt"

val_labels = []

val_file = open(val_label_path, 'r') 
Lines = val_file.readlines() 

for line in Lines[1:]:
  val_labels.append(int(line[-2])) 

val_labels = np.asarray(val_labels)
val_labels = val_labels[val_order] 

print(val_labels)

"""# Uniformize data

# Training data
"""

train_uniform_images = []
img_size = 64
train_images = train_data[0]

for img in train_images:
      image_bgr = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
      image_resized = cv2.resize(image_bgr,(img_size,img_size),interpolation = cv2.INTER_AREA)
      train_uniform_images.append(image_resized)


train_uniform_images = np.array(train_uniform_images)

"""# Validation data"""

val_uniform_images = []
val_images = val_data[0]

for img in val_images:
      image_bgr = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
      image_resized = cv2.resize(image_bgr,(img_size,img_size),interpolation = cv2.INTER_AREA)
      val_uniform_images.append(image_resized)


val_uniform_images = np.array(val_uniform_images)

"""# Test data"""

test_uniform_images = []
test_images = test_data[0]

for img in test_images:
      image_bgr = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)
      image_resized = cv2.resize(image_bgr,(img_size,img_size),interpolation = cv2.INTER_AREA)
      test_uniform_images.append(image_resized)


test_uniform_images = np.array(test_uniform_images)

"""# Display one image"""

print(train_uniform_images.shape)

image_idx = 502

img_post_uniform = train_uniform_images[image_idx]
print(train_names[image_idx])
print(train_labels[image_idx])
plt.imshow(img_post_uniform, cmap=plt.cm.binary)

"""# Build CNN"""

activation_function = "relu"#"relu" "sigmoid" "softmax" "softplus" "softsign" "tanh" "selu" "elu" "exponential"

model = models.Sequential()
model.add(layers.Conv2D(16, (3, 3), activation=activation_function, input_shape=(img_size, img_size, 3)))
model.add(layers.Conv2D(16, (3, 3), activation=activation_function,padding="same"))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(32, (3, 3), activation=activation_function,padding="same"))
model.add(layers.Conv2D(32, (3, 3), activation=activation_function,padding="same"))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation=activation_function,padding="same"))
model.add(layers.Conv2D(64, (3, 3), activation=activation_function,padding="same"))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation=activation_function,padding="same"))
model.add(layers.Conv2D(256, (3, 3), activation=activation_function,padding="same"))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(2304, activation='relu'))
model.add(layers.Dropout(0.4))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.4))
model.add(layers.Dense(7, activation='sigmoid'))

model.summary()

"""# Compile and Train"""

train_images = train_uniform_images
val_images = val_uniform_images
test_images = test_uniform_images

model.compile(optimizer='adamax',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=20, validation_data=(val_images, val_labels))

"""# Evaluate"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(val_images, val_labels, verbose=2)

"""# Prediction on Sample Test data"""

test_results = np.argmax(model.predict(test_images),-1)

img_idx = 174 # choose image 0-2874
print(test_results[img_idx])
img = test_images[img_idx]
plt.imshow(img)

plt.figure(figsize=(20,20))
for i in range(10):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(test_images[i], cmap=plt.cm.binary)
    plt.xlabel(test_results[i], fontsize=18)
plt.show()